{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scored data to: mock-data-scored-454939c3.json\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'capability_group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7dccf437e614>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;31m# Create a pivot table for the heatmap including the environment attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m \u001b[0mheatmap_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_50_riskiest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'capability_group'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Environment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'OverallRiskScore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;31m# Prepare data for hover tooltips\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mpivot_table\u001b[1;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m   8577\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpivot_table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8579\u001b[1;33m         return pivot_table(\n\u001b[0m\u001b[0;32m   8580\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8581\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py\u001b[0m in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pivot_table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     table = __internal_pivot_table(\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py\u001b[0m in \u001b[0;36m__internal_pivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m     \u001b[0mgrouped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobserved\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m     \u001b[0magged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maggfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   8250\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8252\u001b[1;33m         return DataFrameGroupBy(\n\u001b[0m\u001b[0;32m   8253\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8254\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgrouper\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[0;32m    932\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m    983\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;31m# Add key to exclusions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'capability_group'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import streamlit as st\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load the mapping data from the specified path\n",
    "mapping_file_path = r\"C:\\Users\\SURIYA HAASINI\\Downloads\\aws-09.21.2021_attack-9.0-enterprise_json_bkp.json\"\n",
    "with open(mapping_file_path) as f:\n",
    "    mapping_data = json.load(f)\n",
    "\n",
    "# Load the mock data from the specified path\n",
    "mock_data_file_path = r\"C:\\Users\\SURIYA HAASINI\\Downloads\\mock-data-53bdd4d3.json\"\n",
    "with open(mock_data_file_path) as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "# Extract the risk scores and comments from the mapping data\n",
    "risk_scores = {}\n",
    "for obj in mapping_data['mapping_objects']:\n",
    "    risk_scores[obj['attack_object_name']] = {\n",
    "        \"score_value\": obj['score_value'],\n",
    "        \"comments\": obj['comments']\n",
    "    }\n",
    "\n",
    "# Define risk score values\n",
    "score_values = {\n",
    "    \"minimal\": 1,\n",
    "    \"partial\": 2,\n",
    "    \"significant\": 3\n",
    "}\n",
    "\n",
    "# Function to calculate the likelihood score based on risks\n",
    "def calculate_likelihood(risks):\n",
    "    likelihood_score = 0\n",
    "    high_likelihood_count = 0\n",
    "    reasons = []\n",
    "\n",
    "    for risk in risks:\n",
    "        risk_info = risk_scores.get(risk[\"risk\"], {\"score_value\": \"minimal\", \"comments\": \"No specific score available.\"})\n",
    "        score = score_values.get(risk_info[\"score_value\"], 1)\n",
    "        likelihood_score += score\n",
    "        reasons.append(f\"{risk['risk']}: {risk_info['comments']} (Score: {score})\")\n",
    "        if score >= 3:\n",
    "            high_likelihood_count += 1\n",
    "\n",
    "    # Adjust for combinations of high likelihood risks\n",
    "    if high_likelihood_count > 1:\n",
    "        likelihood_score += high_likelihood_count  # Add weight for multiple high likelihood risks\n",
    "\n",
    "    return likelihood_score, reasons\n",
    "\n",
    "# Function to calculate the impact score based on asset attributes\n",
    "def calculate_impact(record):\n",
    "    score = 0\n",
    "\n",
    "    # Environment impact\n",
    "    environment_mapping = {\n",
    "        \"Development\": 1,\n",
    "        \"Testing\": 2,\n",
    "        \"Production\": 3\n",
    "    }\n",
    "    score += environment_mapping.get(record[\"Environment\"], 1)\n",
    "\n",
    "    # Compliance impact\n",
    "    compliance_mapping = {\n",
    "        \"None\": 1,\n",
    "        \"GDPR\": 3,\n",
    "        \"HIPAA\": 2,\n",
    "        \"PCI-DSS\": 2\n",
    "    }\n",
    "    score += compliance_mapping.get(record[\"Compliance\"], 1)\n",
    "\n",
    "    # Data classification impact\n",
    "    classification_mapping = {\n",
    "        \"Public\": 1,\n",
    "        \"Internal\": 2,\n",
    "        \"Confidential\": 3,\n",
    "        \"Restricted\": 4\n",
    "    }\n",
    "    score += classification_mapping.get(record[\"DataClassification\"], 1)\n",
    "\n",
    "    # Security level impact\n",
    "    security_level_mapping = {\n",
    "        \"Low\": 1,\n",
    "        \"Medium\": 2,\n",
    "        \"High\": 3\n",
    "    }\n",
    "    score += security_level_mapping.get(record[\"SecurityLevel\"], 1)\n",
    "\n",
    "    # Adjust for high impact risks\n",
    "    if any(risk[\"risk\"] in {\"Service Exhaustion Flood\", \"Application Exhaustion Flood\", \"Unsecured Credentials\"} for risk in record[\"risks\"]):\n",
    "        score += 2  # Add weight for high impact risks\n",
    "\n",
    "    return score\n",
    "\n",
    "# Function to summarize the reasoning behind the scoring\n",
    "def generate_summary(record, likelihood, impact, reasons):\n",
    "    reason_str = \" \".join(reasons)\n",
    "    return (f\"The asset has a likelihood score of {likelihood} based on its risks and an impact score of {impact} \"\n",
    "            f\"due to its environment being '{record['Environment']}', compliance with '{record['Compliance']}', \"\n",
    "            f\"data classification as '{record['DataClassification']}', and security level of '{record['SecurityLevel']}'. \"\n",
    "            f\"Risk analysis: {reason_str}\")\n",
    "\n",
    "# Update records with scores and summary\n",
    "for record in records:\n",
    "    likelihood_score, reasons = calculate_likelihood(record[\"risks\"])\n",
    "    impact_score = calculate_impact(record)\n",
    "    overall_risk_score = likelihood_score + impact_score\n",
    "    summary = generate_summary(record, likelihood_score, impact_score, reasons)\n",
    "\n",
    "    record.update({\n",
    "        \"LikelihoodScore\": likelihood_score,\n",
    "        \"ImpactScore\": impact_score,\n",
    "        \"OverallRiskScore\": overall_risk_score,\n",
    "        \"Summary\": summary\n",
    "    })\n",
    "\n",
    "# Generate a random hash for the filename suffix\n",
    "hash_suffix = hashlib.md5(os.urandom(16)).hexdigest()[:8]\n",
    "filename = f\"mock-data-scored-{hash_suffix}.json\"\n",
    "\n",
    "# Save the updated records to a new JSON file\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(records, f, indent=2)\n",
    "\n",
    "# Output the filename for reference\n",
    "print(f\"Saved scored data to: {filename}\")\n",
    "\n",
    "# Load the JSON data into a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Convert risks column to a string for display, handling None values\n",
    "def convert_risks(risks):\n",
    "    return ', '.join([risk['risk'] for risk in risks if risk['risk'] is not None])\n",
    "\n",
    "df['risks'] = df['risks'].apply(convert_risks)\n",
    "\n",
    "# Find the top 10 riskiest assets\n",
    "top_10_riskiest = df.nlargest(10, 'OverallRiskScore')\n",
    "\n",
    "# Find the top 50 riskiest assets\n",
    "top_50_riskiest = df.nlargest(50, 'OverallRiskScore')\n",
    "\n",
    "# Create a pivot table for the heatmap including the environment attribute\n",
    "heatmap_data = top_50_riskiest.pivot_table(index='capability_group', columns='Environment', values='OverallRiskScore', aggfunc='mean', fill_value=0)\n",
    "\n",
    "# Prepare data for hover tooltips\n",
    "hover_text = top_50_riskiest.groupby(['capability_group', 'Environment'])['risks'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "hover_data = heatmap_data.copy()\n",
    "for row in hover_data.index:\n",
    "    for col in hover_data.columns:\n",
    "        risk_details = hover_text[(hover_text['capability_group'] == row) & (hover_text['Environment'] == col)]['risks'].values\n",
    "        if risk_details:\n",
    "            hover_data.at[row, col] = risk_details[0]\n",
    "        else:\n",
    "            hover_data.at[row, col] = \"\"\n",
    "\n",
    "# Create the heatmap with Plotly\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=heatmap_data.values,\n",
    "    x=heatmap_data.columns,\n",
    "    y=heatmap_data.index,\n",
    "    colorscale='spectral',\n",
    "    text=hover_data.values,\n",
    "    hoverinfo='text',\n",
    "    hovertemplate='%{text}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Heatmap of Top 50 Riskiest Assets by Asset Type and Environment',\n",
    "    xaxis_nticks=36,\n",
    "    xaxis_title='Environment',\n",
    "    yaxis_title='Capability Group'\n",
    ")\n",
    "\n",
    "# Streamlit app\n",
    "st.title('Risk Analysis Dashboard')\n",
    "\n",
    "st.header('Top 10 Riskiest Assets')\n",
    "st.dataframe(top_10_riskiest)\n",
    "\n",
    "st.header('Heatmap of Top 50 Riskiest Assets by Asset Type and Environment')\n",
    "st.plotly_chart(fig)\n",
    "\n",
    "st.header('Details of Top 50 Riskiest Assets')\n",
    "st.dataframe(top_50_riskiest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlitNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading streamlit-1.39.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\anaconda3\\lib\\site-packages (from streamlit) (5.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Error parsing dependencies of pyodbc: Invalid version: '4.0.0-unsupported'\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 105, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 96, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 483, in run\n",
      "    installed_versions[distribution.canonical_name] = distribution.version\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_internal\\metadata\\pkg_resources.py\", line 192, in version\n",
      "    return parse_version(self._dist.version)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\packaging\\version.py\", line 56, in parse\n",
      "    return Version(version)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\packaging\\version.py\", line 202, in __init__\n",
      "    raise InvalidVersion(f\"Invalid version: '{version}'\")\n",
      "pip._vendor.packaging.version.InvalidVersion: Invalid version: '4.0.0-unsupported'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click<9,>=7.0 in c:\\anaconda3\\lib\\site-packages (from streamlit) (7.1.2)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in c:\\anaconda3\\lib\\site-packages (from streamlit) (1.24.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\anaconda3\\lib\\site-packages (from streamlit) (20.4)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\anaconda3\\lib\\site-packages (from streamlit) (2.0.3)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\anaconda3\\lib\\site-packages (from streamlit) (7.2.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\anaconda3\\lib\\site-packages (from streamlit) (4.25.5)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-17.0.0-cp38-cp38-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting requests<3,>=2.27 (from streamlit)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Downloading rich-13.9.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\anaconda3\\lib\\site-packages (from streamlit) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\anaconda3\\lib\\site-packages (from streamlit) (4.5.0)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\anaconda3\\lib\\site-packages (from streamlit) (6.0.4)\n",
      "Collecting watchdog<6,>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-4.0.2-py3-none-win_amd64.whl.metadata (38 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (2.11.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.2.0)\n",
      "Collecting narwhals>=1.5.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.9.1-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting typing-extensions<5,>=4.3.0 (from streamlit)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\anaconda3\\lib\\site-packages (from packaging<25,>=20->streamlit) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from packaging<25,>=20->streamlit) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2020.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.27->streamlit)\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.7.4)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (1.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.16.0)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (49.2.0.post20200714)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading streamlit-1.39.0-py2.py3-none-any.whl (8.7 MB)\n",
      "   ---------------------------------------- 8.7/8.7 MB 666.7 kB/s eta 0:00:00\n",
      "Downloading altair-5.4.1-py3-none-any.whl (658 kB)\n",
      "   -------------------------------------- 658.1/658.1 kB 704.6 kB/s eta 0:00:00\n",
      "Downloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading pyarrow-17.0.0-cp38-cp38-win_amd64.whl (25.2 MB)\n",
      "   ---------------------------------------- 25.2/25.2 MB 515.1 kB/s eta 0:00:00\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 6.9/6.9 MB 426.5 kB/s eta 0:00:00\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading rich-13.9.2-py3-none-any.whl (242 kB)\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading watchdog-4.0.2-py3-none-win_amd64.whl (82 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl (99 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading narwhals-1.9.1-py3-none-any.whl (180 kB)\n",
      "Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 1.2/1.2 MB 642.2 kB/s eta 0:00:00\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: watchdog, typing-extensions, tenacity, smmap, pygments, pyarrow, narwhals, mdurl, charset-normalizer, blinker, requests, pydeck, markdown-it-py, gitdb, rich, gitpython, altair, streamlit\n",
      "  Attempting uninstall: watchdog\n",
      "    Found existing installation: watchdog 0.10.3\n",
      "    Uninstalling watchdog-0.10.3:\n",
      "      Successfully uninstalled watchdog-0.10.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.6.1\n",
      "    Uninstalling Pygments-2.6.1:\n",
      "      Successfully uninstalled Pygments-2.6.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.24.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\anaconda3\\lib\\site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: packaging in c:\\anaconda3\\lib\\site-packages (from plotly) (20.4)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pyparsing>=2.0.2 in c:\\anaconda3\\lib\\site-packages (from packaging->plotly) (2.4.7)\n",
      "\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from packaging->plotly) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Error parsing dependencies of pyodbc: Invalid version: '4.0.0-unsupported'\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 105, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 96, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 483, in run\n",
      "    installed_versions[distribution.canonical_name] = distribution.version\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_internal\\metadata\\pkg_resources.py\", line 192, in version\n",
      "    return parse_version(self._dist.version)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\packaging\\version.py\", line 56, in parse\n",
      "    return Version(version)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\packaging\\version.py\", line 202, in __init__\n",
      "    raise InvalidVersion(f\"Invalid version: '{version}'\")\n",
      "pip._vendor.packaging.version.InvalidVersion: Invalid version: '4.0.0-unsupported'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading plotly-5.24.1-py3-none-any.whl (19.1 MB)\n",
      "   ---------------------------------------- 19.1/19.1 MB 294.2 kB/s eta 0:00:00\n",
      "Installing collected packages: plotly\n"
     ]
    }
   ],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-5dcac5ab0e91>:227: DeprecationWarning:\n",
      "\n",
      "The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "\n",
      "2024-10-08 19:31:41.034 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.036 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.037 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.037 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.046 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.047 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.048 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.048 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.052 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.053 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.054 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.055 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.056 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.058 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.062 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-08 19:31:41.064 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scored data to: mock-data-scored-34e4191e.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Sample mapping data\n",
    "mapping_data = {\n",
    "    \"mapping_objects\": [\n",
    "        {\n",
    "            \"attack_object_name\": \"Service Exhaustion Flood\",\n",
    "            \"score_value\": \"significant\",\n",
    "            \"comments\": \"High likelihood of service disruption.\"\n",
    "        },\n",
    "        {\n",
    "            \"attack_object_name\": \"Application Exhaustion Flood\",\n",
    "            \"score_value\": \"significant\",\n",
    "            \"comments\": \"Critical impact on application performance.\"\n",
    "        },\n",
    "        {\n",
    "            \"attack_object_name\": \"Unsecured Credentials\",\n",
    "            \"score_value\": \"partial\",\n",
    "            \"comments\": \"Credentials exposed to unauthorized access.\"\n",
    "        },\n",
    "        {\n",
    "            \"attack_object_name\": \"Data Breach\",\n",
    "            \"score_value\": \"significant\",\n",
    "            \"comments\": \"Severe risk of data theft.\"\n",
    "        },\n",
    "        {\n",
    "            \"attack_object_name\": \"Malware Injection\",\n",
    "            \"score_value\": \"minimal\",\n",
    "            \"comments\": \"Low likelihood of success.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Sample records data\n",
    "records = [\n",
    "    {\n",
    "        \"Environment\": \"Production\",\n",
    "        \"Compliance\": \"GDPR\",\n",
    "        \"DataClassification\": \"Confidential\",\n",
    "        \"SecurityLevel\": \"High\",\n",
    "        \"risks\": [\n",
    "            {\"risk\": \"Service Exhaustion Flood\"},\n",
    "            {\"risk\": \"Data Breach\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"Environment\": \"Development\",\n",
    "        \"Compliance\": \"None\",\n",
    "        \"DataClassification\": \"Public\",\n",
    "        \"SecurityLevel\": \"Low\",\n",
    "        \"risks\": [\n",
    "            {\"risk\": \"Malware Injection\"},\n",
    "            {\"risk\": \"Application Exhaustion Flood\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"Environment\": \"Testing\",\n",
    "        \"Compliance\": \"PCI-DSS\",\n",
    "        \"DataClassification\": \"Restricted\",\n",
    "        \"SecurityLevel\": \"Medium\",\n",
    "        \"risks\": [\n",
    "            {\"risk\": \"Unsecured Credentials\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"Environment\": \"Production\",\n",
    "        \"Compliance\": \"HIPAA\",\n",
    "        \"DataClassification\": \"Confidential\",\n",
    "        \"SecurityLevel\": \"High\",\n",
    "        \"risks\": [\n",
    "            {\"risk\": \"Data Breach\"},\n",
    "            {\"risk\": \"Service Exhaustion Flood\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"Environment\": \"Development\",\n",
    "        \"Compliance\": \"None\",\n",
    "        \"DataClassification\": \"Internal\",\n",
    "        \"SecurityLevel\": \"Low\",\n",
    "        \"risks\": [\n",
    "            {\"risk\": \"Application Exhaustion Flood\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract the risk scores and comments from the mapping data\n",
    "risk_scores = {}\n",
    "for obj in mapping_data['mapping_objects']:\n",
    "    risk_scores[obj['attack_object_name']] = {\n",
    "        \"score_value\": obj['score_value'],\n",
    "        \"comments\": obj['comments']\n",
    "    }\n",
    "\n",
    "# Define risk score values\n",
    "score_values = {\n",
    "    \"minimal\": 1,\n",
    "    \"partial\": 2,\n",
    "    \"significant\": 3\n",
    "}\n",
    "\n",
    "# Function to calculate the likelihood score based on risks\n",
    "def calculate_likelihood(risks):\n",
    "    likelihood_score = 0\n",
    "    high_likelihood_count = 0\n",
    "    reasons = []\n",
    "\n",
    "    for risk in risks:\n",
    "        risk_info = risk_scores.get(risk[\"risk\"], {\"score_value\": \"minimal\", \"comments\": \"No specific score available.\"})\n",
    "        score = score_values.get(risk_info[\"score_value\"], 1)\n",
    "        likelihood_score += score\n",
    "        reasons.append(f\"{risk['risk']}: {risk_info['comments']} (Score: {score})\")\n",
    "        if score >= 3:\n",
    "            high_likelihood_count += 1\n",
    "\n",
    "    # Adjust for combinations of high likelihood risks\n",
    "    if high_likelihood_count > 1:\n",
    "        likelihood_score += high_likelihood_count  # Add weight for multiple high likelihood risks\n",
    "\n",
    "    return likelihood_score, reasons\n",
    "\n",
    "# Function to calculate the impact score based on asset attributes\n",
    "def calculate_impact(record):\n",
    "    score = 0\n",
    "\n",
    "    # Environment impact\n",
    "    environment_mapping = {\n",
    "        \"Development\": 1,\n",
    "        \"Testing\": 2,\n",
    "        \"Production\": 3\n",
    "    }\n",
    "    score += environment_mapping.get(record[\"Environment\"], 1)\n",
    "\n",
    "    # Compliance impact\n",
    "    compliance_mapping = {\n",
    "        \"None\": 1,\n",
    "        \"GDPR\": 3,\n",
    "        \"HIPAA\": 2,\n",
    "        \"PCI-DSS\": 2\n",
    "    }\n",
    "    score += compliance_mapping.get(record[\"Compliance\"], 1)\n",
    "\n",
    "    # Data classification impact\n",
    "    classification_mapping = {\n",
    "        \"Public\": 1,\n",
    "        \"Internal\": 2,\n",
    "        \"Confidential\": 3,\n",
    "        \"Restricted\": 4\n",
    "    }\n",
    "    score += classification_mapping.get(record[\"DataClassification\"], 1)\n",
    "\n",
    "    # Security level impact\n",
    "    security_level_mapping = {\n",
    "        \"Low\": 1,\n",
    "        \"Medium\": 2,\n",
    "        \"High\": 3\n",
    "    }\n",
    "    score += security_level_mapping.get(record[\"SecurityLevel\"], 1)\n",
    "\n",
    "    # Adjust for high impact risks\n",
    "    if any(risk[\"risk\"] in {\"Service Exhaustion Flood\", \"Application Exhaustion Flood\", \"Unsecured Credentials\"} for risk in record[\"risks\"]):\n",
    "        score += 2  # Add weight for high impact risks\n",
    "\n",
    "    return score\n",
    "\n",
    "# Function to summarize the reasoning behind the scoring\n",
    "def generate_summary(record, likelihood, impact, reasons):\n",
    "    reason_str = \" \".join(reasons)\n",
    "    return (f\"The asset has a likelihood score of {likelihood} based on its risks and an impact score of {impact} \"\n",
    "            f\"due to its environment being '{record['Environment']}', compliance with '{record['Compliance']}', \"\n",
    "            f\"data classification as '{record['DataClassification']}', and security level of '{record['SecurityLevel']}'. \"\n",
    "            f\"Risk analysis: {reason_str}\")\n",
    "\n",
    "# Update records with scores and summary\n",
    "for record in records:\n",
    "    likelihood_score, reasons = calculate_likelihood(record[\"risks\"])\n",
    "    impact_score = calculate_impact(record)\n",
    "    overall_risk_score = likelihood_score + impact_score\n",
    "    summary = generate_summary(record, likelihood_score, impact_score, reasons)\n",
    "\n",
    "    record.update({\n",
    "        \"LikelihoodScore\": likelihood_score,\n",
    "        \"ImpactScore\": impact_score,\n",
    "        \"OverallRiskScore\": overall_risk_score,\n",
    "        \"Summary\": summary\n",
    "    })\n",
    "\n",
    "# Generate a random hash for the filename suffix\n",
    "hash_suffix = hashlib.md5(os.urandom(16)).hexdigest()[:8]\n",
    "filename = f\"mock-data-scored-{hash_suffix}.json\"\n",
    "\n",
    "# Save the updated records to a new JSON file\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(records, f, indent=2)\n",
    "\n",
    "# Output the filename for reference\n",
    "print(f\"Saved scored data to: {filename}\")\n",
    "\n",
    "# Load the JSON data into a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Convert risks column to a string for display, handling None values\n",
    "def convert_risks(risks):\n",
    "    return ', '.join([risk['risk'] for risk in risks if risk['risk'] is not None])\n",
    "\n",
    "df['risks'] = df['risks'].apply(convert_risks)\n",
    "\n",
    "# Find the top 10 riskiest assets\n",
    "top_10_riskiest = df.nlargest(10, 'OverallRiskScore')\n",
    "\n",
    "# Find the top 50 riskiest assets\n",
    "top_50_riskiest = df.nlargest(50, 'OverallRiskScore')\n",
    "\n",
    "# Create a pivot table for the heatmap including the environment attribute\n",
    "heatmap_data = top_50_riskiest.pivot_table(index='DataClassification', columns='Environment', values='OverallRiskScore', aggfunc='mean', fill_value=0)\n",
    "\n",
    "# Prepare data for hover tooltips\n",
    "hover_text = top_50_riskiest.groupby(['DataClassification', 'Environment'])['risks'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "hover_data = heatmap_data.copy()\n",
    "for row in hover_data.index:\n",
    "    for col in hover_data.columns:\n",
    "        risk_details = hover_text[(hover_text['DataClassification'] == row) & (hover_text['Environment'] == col)]['risks'].values\n",
    "        if risk_details:\n",
    "            hover_data.at[row, col] = risk_details[0]\n",
    "        else:\n",
    "            hover_data.at[row, col] = \"\"\n",
    "\n",
    "# Create the heatmap with Plotly\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=heatmap_data.values,\n",
    "    x=heatmap_data.columns,\n",
    "    y=heatmap_data.index,\n",
    "    colorscale='spectral',\n",
    "    text=hover_data.values,\n",
    "    hoverinfo='text',\n",
    "    hovertemplate='%{text}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Heatmap of Top 50 Riskiest Assets by Data Classification and Environment',\n",
    "    xaxis_nticks=36,\n",
    "    xaxis_title='Environment',\n",
    "    yaxis_title='Data Classification'\n",
    ")\n",
    "\n",
    "# Streamlit app\n",
    "st.title('Risk Analysis Dashboard')\n",
    "\n",
    "st.header('Top 10 Riskiest Assets')\n",
    "st.dataframe(top_10_riskiest)\n",
    "\n",
    "st.header('Heatmap of Top 50 Riskiest Assets by Data Classification and Environment')\n",
    "st.plotly_chart(fig)\n",
    "\n",
    "st.header('Details of Top 50 Riskiest Assets')\n",
    "st.dataframe(top_50_riskiest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

